{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitd0b078c581744c51ba9098f8d851824d",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.set_printoptions(linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data/FashionMNIST'\n",
    "    ,train=True\n",
    "    ,download=True\n",
    "    ,transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = F.max_pool2d(F.relu(self.conv1(t)), kernel_size=2, stride=2)\n",
    "\n",
    "        t = F.max_pool2d(F.relu(self.conv2(t)), kernel_size=2, stride=2)\n",
    "\n",
    "        t = F.relu(self.fc1(t.reshape(-1, 12*4*4)))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "\n",
    "        return t\n",
    "\n",
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0 loss: 329.2417930960655 total correct: 47723\n"
    }
   ],
   "source": [
    "network = Network()\n",
    "batch = next(iter(train_loader))\n",
    "optimiser = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1):\n",
    "\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for batch in train_loader: #looping each image in the batch\n",
    "        images, labels = batch\n",
    "\n",
    "        preds = network(images)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += num_correct(preds, labels)\n",
    "\n",
    "    print('epoch:', epoch , 'loss:', total_loss, 'total correct:', total_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_preds(model, loader):\n",
    "    all_preds = torch.tensor([])\n",
    "\n",
    "    for batch in loader:\n",
    "        images, labels = batch\n",
    "        preds = network(images)\n",
    "\n",
    "        all_preds = torch.cat((all_preds, preds), dim=0)\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_loader = torch.utils.data.DataLoader(train_set, batch_size=5000)\n",
    "train_preds = get_preds(network, pred_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have initliased a function to recieve all the prediction tensor values for the entire training set\n",
    "\n",
    "this has been done in order to graph the predictions for each label, here we can identify areas in which the model is mistaking items with each other\n",
    "\n",
    "the @torch.no_grad() decorator has been added in order to prevent pytorch for tracking the gradient of the values within the prediction tensor, as it is not necessary for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([60000, 10])"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "train_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "total correct: 49898\ntotal accuracy: 0.8316333333333333\n"
    }
   ],
   "source": [
    "preds_correct = num_correct(train_preds, train_set.targets)\n",
    "\n",
    "print('total correct:', preds_correct)\n",
    "print('total accuracy:', preds_correct / len(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_set.targets represents all the labels for the whole training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = torch.stack((train_set.targets, train_preds.argmax(dim=1)), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this stacked tensor is a pairing of the dataset labels and the predicted labels, allowing us to perform an operation finding incorrect pairing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = torch.zeros(10, 10, dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in stacked:\n",
    "    tl, pl = n.tolist()\n",
    "    confusion_matrix[tl, pl] = confusion_matrix[tl, pl] +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[26659,     5,    41,  2431,    40,   178,   374,  1327,    96,   410],\n        [   24,  5628,     7,   291,    16,     2,    22,     0,    10,     0],\n        [   85,     3, 15437,    61,  2081,   310,   795,     0,   110,     6],\n        [ 1319,     9,    21, 15098,   590,    53,    86,   511,    10,   303],\n        [   11,     6,   324,    88, 10969,   139,   424,     0,    39,     0],\n        [   85,     0,   424,    72,   402, 16379,     0,   923,    27,   160],\n        [ 1406,     4,   489,   146,   804,     1,  2933,     0,   216,     1],\n        [ 1443,     0,     1,   950,     0,   222,     0,  8790,    10,  1534],\n        [   17,     1,     9,    21,    41,    12,    36,    25,  5837,     1],\n        [  268,     0,     8,   239,     0,   106,     1,   751,     9, 10849]], dtype=torch.int32)"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this confusion matrix shows the prediction tensors number of estimates for each label, moving down the diagonal, there are higher values as these are the correctly predicted images"
   ]
  }
 ]
}